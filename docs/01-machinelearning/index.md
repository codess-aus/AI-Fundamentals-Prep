# ğŸ¤–ğŸ’¡ Whatâ€™s Machine Learning?
Machine learning is like when computers learn from data so they can make smart guesses â€” kind of like how we learn from experience!

![Machine Learning](../assets/3.%20Machine%20Learning.png)

## ğŸ§  Whoâ€™s Involved?
Itâ€™s a team-up between:

- **Data Scientists ğŸ§ª:** They explore and clean up the data, then train the AI to recognize patterns.
- **Software Developers ğŸ’»:** They take that trained AI and plug it into apps so it can make predictions in real life!

## ğŸ” How It Works:
- Feed the AI lots of data ğŸ•ğŸ“Š
- Train it to spot patterns ğŸ§ 
- Use it in apps to make smart predictions â€” like guessing what youâ€™ll type next or spotting a cat in a photo ğŸ±

This last step is called **inferencing** â€” itâ€™s when the AI uses what it learned to make real-time decisions!

---

## ğŸ§  Where It Comes From

ML started with **math and stats** â€” using numbers and patterns to figure things out.  
The big idea? Use past data to guess what might happen next!

---

## ğŸ” Real-Life Examples

- ğŸ¦ **Ice Cream Shop**  
  An app looks at past sales + weather to guess how many ice creams will sell today.

- ğŸ©º **Doctorâ€™s Tool**  
  AI checks patient data to predict if someone might be at risk for diabetes.

- ğŸ§ **Penguin Research**  
  AI uses bird measurements to figure out which species it is â€” Adelie, Gentoo, or Chinstrap!

---

# ğŸ§ª How Machine Learning Works

ML is like building a smart calculator that turns **inputs** into **predictions**.

---
![Machine Learning](../assets/machine-learning.png)

## ğŸ§© Step 1: Training

![Machine Learning](../assets/5.%20Machine%20Learning%20Is.png)

We give the AI a bunch of examples:

- **Features (x)** = the stuff we know (like weather, weight, flipper size)  
- **Label (y)** = the thing we want to predict (like sales, risk level, penguin type)

Example:

x = [temperature, rainfall, windspeed]
y = ice cream sales


The AI learns a function â€” letâ€™s call it **f** â€” that connects x to y:
y = f(x)


---

## ğŸ”® Step 2: Inferencing

Once trained, the AI can take *new* x values and guess y â€” this guess is called **Å·** (aka â€œy-hatâ€ ğŸ©).

Example:
Å· = 120 ice creams ğŸ¦


---

## ğŸ’¡ Summary

- **Machine Learning** = Smart guessing powered by data + math  
- It learns patterns, builds a function, and makes predictions â€” all inside your apps!

---

# ğŸ§  Types of Machine Learning

Machine learning comes in different styles â€” like different game modes ğŸ®. You pick the one that fits what you're trying to predict!

![Machine Learning Types](../assets/machine-learning-types.png)

---

## ğŸ“ Supervised Learning

This is when the AI learns from examples that include both:
- **Features (x)** = the stuff we know
- **Labels (y)** = the answer we want it to learn

### ğŸ”¢ Regression

Predicts a **number** â€” like how many ice creams will sell or how fast a car goes.

Examples:
- ğŸ¦ Ice cream sales based on weather
- ğŸ  House price based on size and location
- ğŸš— Car fuel efficiency based on engine size and weight

### ğŸ§ª Classification

Predicts a **category** â€” like yes/no or which type of penguin ğŸ§.

#### âœ… Binary Classification

Only **two options** â€” like true/false or yes/no.

Examples:
- ğŸ©º Is a patient at risk for diabetes?
- ğŸ’³ Will a customer default on a loan?
- ğŸ“¬ Will someone respond to a marketing email?

#### ğŸ¯ Multiclass Classification

More than **two categories**, but only one correct answer.

Examples:
- ğŸ§ Penguin species: Adelie, Gentoo, or Chinstrap
- ğŸ¬ Movie genre: Comedy, Horror, Romance, etc.

#### ğŸ·ï¸ Multilabel Classification

Sometimes, more than one label fits!

Example:
- ğŸ¥ A movie could be both Sci-Fi *and* Comedy

---

## ğŸ§© Unsupervised Learning

Here, the AI only gets **features (x)** â€” no labels. It figures out patterns on its own!

### ğŸ§  Clustering

Groups similar things together â€” like sorting socks ğŸ§¦ by color and size.

Examples:
- ğŸŒ¸ Grouping flowers by petal size and shape
- ğŸ›ï¸ Segmenting customers by shopping habits

> Clustering is like classification, but without knowing the categories ahead of time.  
> You can even use clustering to *find* categories before training a classifier!

---

## ğŸ” How It All Connects

Sometimes you:
1. Use **clustering** to find patterns
2. Label those patterns
3. Train a **classification model** to predict future labels

---
# ğŸ¤– What is Regression?

Regression is like teaching AI to **guess numbers** based on patterns it sees in past data.  
Itâ€™s part of **supervised learning**, which means the AI learns from examples that include both:
- **Features (x)** = the stuff we know (like temperature)
- **Labels (y)** = the number we want to predict (like ice cream sales)

![Supervised Training](../assets/supervised-training.png)
---

# ğŸ§  How AI Learns to Predict: 4 Simple Steps

## 1. ğŸ² Split the Data
Break your data into two parts:
- **Training data** to teach the AI
- **Validation data** to test how well it learned

---

## 2. ğŸ§  Train the Model
Use an algorithm (like **linear regression**) to find patterns in the training data and build a model.

---

## 3. ğŸ§ª Test the Model
Use the validation data to make predictions and see how close the AIâ€™s guesses (Å·) are to the real answers (y).

---

## 4. ğŸ“ Measure & Improve
Compare the predicted answers to the real ones.  
Use metrics (like MAE, MSE, RMSE, or RÂ²) to see how accurate the model is.  
Then tweak the algorithm or settings and repeat until itâ€™s awesome!

---

And thatâ€™s how AI gets smarter with every loop! ğŸ”

---

## ğŸ“Š Visual Example

Imagine a scatter plot with dots showing how many ice creams were sold at different temperatures.  
The regression line goes through the middle of the dots, showing the trend.

---

# ğŸ§ª Part 2: Evaluating a Regression Model

Once your model is trained, itâ€™s time to test how well it works!

## ğŸ§¾ Step 1: Use the Validation Data

We saved some data earlier â€” now we use it to:
- Make predictions using the model
- Compare those predictions to the real answers

Example:

| Temperature (x) | Actual Sales (y) | Predicted Sales (Å·) |
|-----------------|------------------|----------------------|
| 52              | 0                | 2                    |
| 67              | 14               | 17                   |
| 70              | 23               | 20                   |
| 73              | 22               | 23                   |
| 78              | 26               | 28                   |
| 83              | 36               | 33                   |

---

## ğŸ“ How Do We Measure Accuracy?

### 1. **MAE (Mean Absolute Error)**  
Average of how far off the predictions were â€” no matter if too high or too low.  
ğŸ§® Example: MAE = 2.33 ice creams

### 2. **MSE (Mean Squared Error)**  
Like MAE, but it **squares** the errors â€” so big mistakes count more.  
ğŸ§® Example: MSE = 6

### 3. **RMSE (Root Mean Squared Error)**  
The square root of MSE â€” gives us the error in real units (like ice creams!).  
ğŸ§® Example: RMSE = 2.45 ice creams

### 4. **RÂ² (R-Squared)**  
Tells us how much of the pattern the model explains.  
- 1 = perfect prediction  
- 0 = no better than guessing  
ğŸ§® Example: RÂ² = 0.95 (which is awesome!)

---

## ğŸ” Keep Improving!

To get the best model, data scientists try different things:
- ğŸ§  Choose better features
- ğŸ”§ Try different algorithms
- âš™ï¸ Tweak the settings (called hyperparameters)

They repeat the process until the model is accurate enough to use in the real world!

---
# ğŸ§  Binary Classification

---

## ğŸ¯ Whatâ€™s Binary Classification?

Imagine teaching a robot to answer **yes or no** questions. Thatâ€™s binary classification. Itâ€™s like giving the robot a bunch of examples and asking it to guess if something is **true (1)** or **false (0)**.

Instead of guessing numbers (like in regression), the robot guesses **probabilities** â€” like â€œIâ€™m 70% sure this person has diabetes.â€

---

## ğŸ§ª Training the Robot

We give it data like this:

| Blood Glucose (x) | Diabetic? (y) |
|-------------------|---------------|
| 67                | 0             |
| 103               | 1             |
| 114               | 1             |
| 72                | 0             |
| 116               | 1             |
| 65                | 0             |

Then we use a smart math trick called **logistic regression** to draw an S-shaped curve (called a **sigmoid**) that helps the robot decide.

### ğŸ¤– The Robotâ€™s Brain (Function)

f(x) = P(y = 1 | x)

This means: â€œWhatâ€™s the chance y is 1 (true) given x?â€

If the robot sees a glucose level of 90 and the curve says 0.9 (90%), it predicts **yes** â€” this person probably has diabetes.

---

## ğŸ§ª Testing the Robot

We test it with new data:

| Blood Glucose (x) | Diabetic? (y) |
|-------------------|---------------|
| 66                | 0             |
| 107               | 1             |
| 112               | 1             |
| 71                | 0             |
| 87                | 1             |
| 89                | 1             |

The robot makes predictions (Å·), and we compare them to the real answers (y).

---

## ğŸ“Š Confusion Matrix (aka Scoreboard)

| Actual (y) | Predicted (Å·) | Result         |
|------------|----------------|----------------|
| 0          | 0              | âœ… True Negative |
| 1          | 1              | âœ… True Positive |
| 1          | 1              | âœ… True Positive |
| 0          | 0              | âœ… True Negative |
| 1          | 0              | âŒ False Negative |
| 1          | 1              | âœ… True Positive |

---

## ğŸ“ˆ Metrics That Matter

### ğŸ¯ Accuracy  
How often the robot gets it right:

Accuracy = (TP + TN) / Total = (3 + 2) / 6 = 0.83


### ğŸ” Recall  
How many actual diabetics the robot found:

Recall = TP / (TP + FN) = 3 / (3 + 1) = 0.75


### ğŸ¯ Precision  
How many predicted diabetics were actually diabetic:

Precision = TP / (TP + FP) = 3 / (3 + 0) = 1.0


### ğŸ§® F1-Score  
Combo of precision and recall:

F1 = (2 Ã— Precision Ã— Recall) / (Precision + Recall) = 0.86


---

## ğŸ“‰ ROC Curve & AUC

We plot how good the robot is at guessing across all thresholds. A perfect robot gets an **AUC of 1.0**. Random guessing gets **0.5**.

Our robot? **AUC = 0.875** ğŸ‰  
That means itâ€™s way better than guessing!

---

## ğŸ§  TL;DR

Binary classification is like teaching a robot to say **yes or no** based on data. We train it, test it, and score it using metrics like accuracy, recall, precision, and AUC. The better the scores, the smarter the robot!


# ğŸ§  Multiclass Classification â€“ Level Up Your ML Game ğŸ®  

## ğŸš€ Whatâ€™s the Vibe?

Multiclass classification is like giving your model the power to choose between *more than two* options. Instead of just â€œyesâ€ or â€œno,â€ itâ€™s like â€œAdelie,â€ â€œGentoo,â€ or â€œChinstrap.â€ ğŸ§ Itâ€™s part of the supervised ML squad, just like regression and binary classification. You train it, validate it, and test it â€” classic ML grind.

---

## ğŸ§Š Penguin Example (Because Penguins Are Cool)

Weâ€™ve got penguins, and weâ€™re checking out their flipper lengths (ğŸ“). Based on that, we wanna guess their species:

- `0`: Adelie  
- `1`: Gentoo  
- `2`: Chinstrap  

| Flipper Length (x) | Species (y) |
|--------------------|-------------|
| 167                | 0           |
| 172                | 0           |
| 225                | 2           |
| 197                | 1           |
| 189                | 1           |
| 232                | 2           |
| 158                | 0           |

*Note: Real-world data would have more features, but weâ€™re keeping it chill with just one.*

---

## ğŸ› ï¸ How Do We Train This Thing?

Weâ€™ve got two main ways to train a multiclass model:

### 1. One-vs-Rest (OvR)  
Train a separate binary classifier for each class. Each one says, â€œIs this my class or not?â€

- `f0(x) = P(y=0 | x)`  
- `f1(x) = P(y=1 | x)`  
- `f2(x) = P(y=2 | x)`  

Whichever one gives the highest probability wins. ğŸ†

### 2. Multinomial (aka Softmax Squad)  
One model, one function, all the probs:

```text
f(x) = [P(y=0|x), P(y=1|x), P(y=2|x)]
```

Example output: `[0.2, 0.3, 0.5]` â†’ Class 2 wins.

---

## ğŸ“Š Time to Evaluate

Letâ€™s see how our model did on some new penguins:

| Flipper Length (x) | Actual (y) | Predicted (Å·) |
|--------------------|------------|----------------|
| 165                | 0          | 0              |
| 171                | 0          | 0              |
| 205                | 2          | 1              |
| 195                | 1          | 1              |
| 183                | 1          | 1              |
| 221                | 2          | 2              |
| 214                | 2          | 2              |

### ğŸ” Confusion Matrix Vibes

| Class | TP | TN | FP | FN | Accuracy | Recall | Precision | F1-Score |
|-------|----|----|----|----|----------|--------|-----------|----------|
| 0     | 2  | 5  | 0  | 0  | 1.00     | 1.00   | 1.00      | 1.00     |
| 1     | 2  | 4  | 1  | 0  | 0.86     | 1.00   | 0.67      | 0.80     |
| 2     | 2  | 4  | 0  | 1  | 0.86     | 0.67   | 1.00      | 0.80     |

### ğŸ“ˆ Overall Stats

- **Accuracy:** (13 + 6) Ã· (13 + 6 + 1 + 1) = **0.90**  
- **Recall:** 6 Ã· (6 + 1) = **0.86**  
- **Precision:** 6 Ã· (6 + 1) = **0.86**  
- **F1-Score:** (2 Ã— 0.86 Ã— 0.86) Ã· (0.86 + 0.86) = **0.86**

---

## ğŸ‰ TL;DR

Multiclass classification = your model picking the best label from a bunch. Whether itâ€™s OvR or softmax, itâ€™s all about those probabilities. And when itâ€™s done right? You get stats that slap. ğŸ’¥

# ğŸ§  Clustering â€“ Group Vibes Only ğŸŒ¸  

## ğŸ¤– Whatâ€™s Clustering All About?

![flowers](../assets/flowers.png)

Clustering is like sorting your playlist by vibe instead of genre. ğŸ§ Itâ€™s **unsupervised ML**, which means no labels, no rules â€” just grouping stuff based on how similar it is. The model doesnâ€™t know what the â€œrightâ€ answer is. It just finds patterns and builds squads (aka clusters) based on features.

---

## ğŸŒ¼ Flower Power Example

Imagine youâ€™re a botanist checking out flowers. You count how many **leaves** and **petals** each one has:

| Leaves (xâ‚) | Petals (xâ‚‚) |
|-------------|-------------|
| 0           | 5           |
| 0           | 6           |
| 1           | 3           |
| 1           | 3           |
| 1           | 6           |
| 1           | 8           |
| 2           | 3           |
| 2           | 7           |
| 2           | 8           |

No labels, no species â€” just raw data. The goal? Group similar flowers together based on leaf and petal counts. ğŸŒ¿ğŸŒ¸

---

## ğŸ› ï¸ Training the Model â€“ K-Means Style

One of the go-to clustering algorithms is **K-Means**. Hereâ€™s how it rolls:

1. **Vectorize the features** â†’ Turn each flower into a point in space: `[xâ‚, xâ‚‚]`.
2. **Pick your number of clusters (k)** â†’ Say you want 3 squads, set `k = 3`.
3. **Drop random centroids** â†’ These are the squad leaders.
4. **Assign each flower to the closest centroid** â†’ Squad up!
5. **Move centroids to the center of their squad** â†’ Based on average distance.
6. **Reassign flowers if needed** â†’ If they vibe more with another squad.
7. **Repeat until stable** â†’ Or hit max iterations.

ğŸï¸ Imagine an animation where flowers keep switching squads until everyoneâ€™s chill.

![Clustering](../assets/clustering.gif)
---

## ğŸ“Š Evaluating the Squad Game

Since thereâ€™s no â€œcorrectâ€ label, we judge clustering by how tight and separate the squads are. Hereâ€™s how:

- **Avg distance to cluster center** â†’ How close each flower is to its squad leader.
- **Avg distance to other centers** â†’ How far it is from other squads.
- **Max distance to cluster center** â†’ The outlier in the squad.
- **Silhouette score** â†’ From -1 to 1. Closer to 1 = better squad vibes.

---

## ğŸ‰ TL;DR

Clustering = unsupervised squad building. No labels, just vibes. K-Means helps group similar data points, and we measure how good the squads are by how close and separate they are. ğŸŒˆ

# ğŸ§  Deep Learning â€“ Brain Mode Activated ğŸ§¬  

## ğŸ¤– What Is Deep Learning?

Deep learning is like giving your computer a mini brain. ğŸ§  It uses **artificial neural networks** to learn stuff â€” kinda like how our brains fire signals, but with math instead of electricity.

| ğŸ§¬ Real Brain | ğŸ’» AI Brain |
|--------------|-------------|
| Neurons fire signals | Neurons = math functions with weights |
| Signals pass to other neurons | Outputs pass through activation functions |

These networks have **layers on layers** of neurons â€” thatâ€™s why itâ€™s called *deep* learning. You can use it for all kinds of ML tasks: regression, classification, NLP, computer vision â€” basically, itâ€™s the superhero of ML. ğŸ¦¸â€â™‚ï¸

---

## ğŸ§ Penguin Classifier Example

Letâ€™s say we wanna ID penguin species using deep learning. We feed in some penguin stats:

- Bill length  
- Bill depth  
- Flipper length  
- Weight  

So our input vector is:  
```text
x = [xâ‚, xâ‚‚, xâ‚ƒ, xâ‚„]
```

And weâ€™re trying to predict the species:

- `0`: Adelie  
- `1`: Gentoo  
- `2`: Chinstrap  

The output is a vector of probabilities:  
```text
y = [P(y=0|x), P(y=1|x), P(y=2|x)]
```

### ğŸ§ª Example Run

Input:  
```text
x = [37.3, 16.8, 19.2, 30.0]
```

Output:  
```text
Å· = [0.2, 0.7, 0.1]
```

Prediction: Class `1` â†’ Gentoo ğŸ§

---

## ğŸ§  How Does It Learn?

Training a neural network = teaching it to get better at guessing. Hereâ€™s the vibe:

1. **Start with random weights (w)**  
2. **Feed training data (x) through the network**  
3. **Get predictions (Å·)**  
4. **Compare with actual labels (y)** using a **loss function**  
5. **Calculate how wrong it was**  
6. **Use math magic (gradient descent)** to tweak the weights  
7. **Repeat over multiple rounds (epochs)** until itâ€™s ğŸ”¥

### ğŸ§ª Example Loss

Predicted:  
```text
Å· = [0.3, 0.1, 0.6]
```

Actual:  
```text
y = [0.0, 0.0, 1.0]
```

Loss = difference =  
```text
[0.3, 0.1, 0.4]
```

The model uses this info to adjust weights and get better next time.

---

## âš¡ Pro Tip

Neural networks donâ€™t train one case at a time â€” they batch data into **matrices** and use **linear algebra** to crunch numbers fast. Thatâ€™s why we use **GPUs** â€” theyâ€™re built for this kind of heavy lifting. ğŸ’ª

---

## ğŸ‰ TL;DR

Deep learning = brain-like ML using layers of neurons. It learns by tweaking weights to reduce errors and gets better with each round. Perfect for complex stuff like image recognition, language, and yes â€” penguins. ğŸ§ğŸ’¥


# ğŸ¤– Transformers â€“ The Real MVPs of AI ğŸ’¬  

## ğŸ§  Whatâ€™s the Deal?

Transformers are the tech behind all the cool generative AI stuff â€” like ChatGPT, Bing Chat, and more. Theyâ€™re built for **natural language processing (NLP)** and can do things like:

- ğŸ’¬ Understand vibes (sentiment)
- ğŸ“š Summarize stuff
- ğŸ” Compare meanings
- âœï¸ Generate new text

Theyâ€™re super smart because theyâ€™ve been trained on *massive* amounts of text. The secret sauce? A special architecture called the **Transformer**.

---

## ğŸ—ï¸ Transformer Architecture 101

Transformers have two main parts:

- **Encoder** â†’ Understands the input text  
- **Decoder** â†’ Generates new text  

They break down text into **tokens** (like words or parts of words), then use a technique called **attention** to figure out which words matter most. The result? Super smart predictions that sound human.

### ğŸ” Example Flow

Input: `"When my dog was"`  
Output: `"a puppy"` ğŸ¶

---

## ğŸ§© Tokenization â€“ Breaking It Down

First step: split text into tokens. Example:

```text
"I heard a dog bark loudly at a cat"
```

Tokens:
- I â†’ 1  
- heard â†’ 2  
- a â†’ 3  
- dog â†’ 4  
- bark â†’ 5  
- loudly â†’ 6  
- at â†’ 7  
- cat â†’ 8  

So the sentence becomes:  
```text
{1 2 3 4 5 6 7 3 8}
```

As training continues, more tokens get added:
- meow â†’ 9  
- skateboard â†’ 10  
- â€¦and so on

---

## ğŸ§  Embeddings â€“ Giving Tokens Meaning

Token IDs are cool, but they donâ€™t tell us what words *mean*. Thatâ€™s where **embeddings** come in â€” theyâ€™re vectors (like `[10, 3, 1]`) that represent the *semantic* meaning of a token.

Example:

```text
dog â†’ [10, 3, 2]  
cat â†’ [10, 3, 1]  
puppy â†’ [5, 2, 1]  
skateboard â†’ [-3, 3, 2]
```

Vectors for â€œdog,â€ â€œcat,â€ and â€œpuppyâ€ point in similar directions â†’ theyâ€™re semantically linked. â€œSkateboardâ€? Totally different vibe. ğŸ›¹

---

## ğŸ‘€ Attention â€“ What Words Matter Most?

Attention layers help the model figure out which words in a sentence are most important. Itâ€™s like giving each word a spotlight based on context.

### ğŸ” Self-Attention

In the encoder:  
- â€œbarkâ€ in â€œdog barkâ€ â‰  â€œbarkâ€ in â€œtree barkâ€  
- Context changes meaning â†’ embeddings adapt

In the decoder:  
- Predicts the next word based on whatâ€™s already been said  
- Example:  
  ```text
  "I heard a dog" â†’ next word = "bark"
  ```

### ğŸ§  Positional Encoding

Each token gets a position tag so the model knows the order:
```text
[1,5,6,2] â†’ I  
[2,9,3,1] â†’ heard  
[3,1,1,2] â†’ a  
[4,10,3,2] â†’ dog
```

---

## ğŸ”„ Multi-Head Attention â€“ Supercharged Predictions

Instead of one attention score, the model calculates *multiple* scores using different parts of the embeddings. Then a neural network picks the best next token.

The process repeats:
1. Predict next token  
2. Add it to the sequence  
3. Use the updated sequence to predict the next one  
4. Keep going until the sentence is complete

---

## ğŸ§ª Training vs. Inferencing

During training:
- The model knows the full sentence  
- It masks future tokens and learns by comparing predictions to actual tokens  
- Adjusts weights to reduce errors (loss)

During inferencing:
- The model uses what itâ€™s learned to predict new text  
- No peeking ahead â€” just vibes and math

---

## âœ¨ Why Transformers Are ğŸ”¥

Models like **GPT-4** are trained on *huge* datasets and use this transformer magic to generate text that feels human. Itâ€™s not â€œthinkingâ€ â€” itâ€™s just really good at predicting what comes next.

So when you type a prompt, it responds with a completion that makes sense â€” thanks to embeddings, attention, and a massive vocabulary.

---

## ğŸ‰ TL;DR

Transformers = the brains behind generative AI.  
They tokenize text, embed meaning, use attention to focus, and generate smart responses.  
Itâ€™s not magic â€” itâ€™s math, data, and a whole lotta training. ğŸ’¥

# ğŸ§ª Module Assessment  

### 1ï¸âƒ£ You want to create a model to predict the cost of heating an office building based on its size in square feet and the number of employees working there. What kind of machine learning problem is this?

- [ ] Regression  
- [ ] Classification  
- [ ] Clustering  

---

### 2ï¸âƒ£ You need to evaluate a classification model. Which metric can you use?

- [ ] Mean squared error (MSE)  
- [ ] Precision  
- [ ] Silhouette  

---

### 3ï¸âƒ£ In deep learning, what is the purpose of a loss function?

- [ ] To remove data for which no known label values are provided  
- [ ] To evaluate the aggregate difference between predicted and actual label values  
- [ ] To calculate the cost of training a neural network rather than a statistical model  
