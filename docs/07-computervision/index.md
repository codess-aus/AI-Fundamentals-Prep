# ğŸ‘€ğŸ’¡ Whatâ€™s Computer Vision? Letâ€™s Break It Down

![Computer Vision](../assets/6.%20Computer%20Vision.png)

Computer vision is like giving AI a pair of digital eyes. It helps machines *see* the world and actually *understand* whatâ€™s going on. No eyeballs neededâ€”just pixels and brainpower.

---

## ğŸ” Real-Life Scenarios Where CV Pops Off

- ğŸ¥ **Hospitals**  
  Track surgical tools in real-time during operations. No more â€œwhereâ€™s that scalpel?â€ moments.

- ğŸ›ï¸ **Retail**  
  Auto-sort product picsâ€”shoes, shirts, gadgetsâ€”into categories. Instant inventory magic.

- ğŸ¾ **Wildlife Watchers**  
  Spot animals in video footage. Think: â€œThatâ€™s a tiger!â€ not â€œWhatâ€™s that blur?â€

- ğŸš— **City Traffic Teams**  
  Read license plates from images. Fast, accurate, and no human squinting required.

- ğŸ­ **Manufacturing**  
  Scan for defects in visual patterns. Catch the flaws before they hit the shelves.

---

## ğŸ§  How It Works

Okay, so computers donâ€™t have eyes like us. But they *do* have the power to process imagesâ€”whether itâ€™s from a live camera feed or a saved photo/video.

This image-processing skill is the secret sauce behind software that mimics human vision. Itâ€™s how AI starts to *see* and *think* visually.

![Computer Vision](../assets/7.%20Computer%20Vision.png)

---

## ğŸ”§ Whatâ€™s Next?

In this module, weâ€™ll dive into the building blocks of modern computer vision. Get ready to explore how machines turn pixels into perception.

# ğŸ‘ï¸ğŸ’¥ Computer Vision â€” What It Can Do & How It Sees

Computer vision is like giving AI supercharged digital eyes. It doesnâ€™t just lookâ€”it *understands*. Letâ€™s break down the main types of CV powers:

![Computer Vision](../assets/13.%20Computer%20Vision.png)

## ğŸ§  Core CV Capabilities

| Type                     | What It Does                                                                 |
|--------------------------|------------------------------------------------------------------------------|
| ğŸ–¼ï¸ Image Analysis         | Detects, classifies, captions, and pulls insights from pics.                 |
| ğŸ“ Spatial Analysis       | Tracks peopleâ€™s movements IRL in real time.                                 |
| ğŸ§‘ Facial Recognition     | Knows whoâ€™s whoâ€”verifies identity like a boss.                              |
| ğŸ”¤ OCR (Text Detection)   | Reads printed or handwritten text in any style or language.                 |

---

## ğŸ–¼ï¸ How AI Sees â€” Itâ€™s All Pixels, Baby

To a computer, an image = a grid of numbers. Each number = a pixel value. Here's a 7x7 grayscale example:

0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 255 255 255 0 0 0 0 255 255 255 0 0 0 0 255 255 255 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0


- `0` = black  
- `255` = white  
- Anything in between = shades of gray

This is a **grayscale image**. But most images are in **color**, which means 3 layers: **Red**, **Green**, and **Blue** (aka RGB).

---

## ğŸ¨ RGB Breakdown â€” Color Vibes

Hereâ€™s how a color image is built using RGB channels:

### ğŸ”´ Red Channel:
 150  150  150  150  150  150  150  
 150  150  150  150  150  150  150
 150  150  255  255  255  150  150
 150  150  255  255  255  150  150
 150  150  255  255  255  150  150
 150  150  150  150  150  150  150
 150  150  150  150  150  150  150

### ğŸŸ¢ Green Channel:
 0    0    0    0    0    0    0          
 0    0    0    0    0    0    0
 0    0   255  255  255   0    0
 0    0   255  255  255   0    0
 0    0   255  255  255   0    0
 0    0    0    0    0    0    0
 0    0    0    0    0    0    0

### ğŸ”µ Blue Channel
 255  255  255  255  255  255  255  
 255  255  255  255  255  255  255
 255  255   0    0    0   255  255
 255  255   0    0    0   255  255
 255  255   0    0    0   255  255
 255  255  255  255  255  255  255
 255  255  255  255  255  255  255

Here's what it looks like:

![Colour Square](../assets/color-square.png)

## ğŸ¨ Color Combos

- **Purple Squares** =  
  - Red: 150  
  - Green: 0  
  - Blue: 255  

- **Yellow Squares (Center)** =  
  - Red: 255  
  - Green: 255  
  - Blue: 0  

---

## ğŸ”œ Whatâ€™s Next?

Now that we know how AI sees, letâ€™s explore how it *processes* those images to make smart decisions.

# ğŸ§  Understand Image Processing

## ğŸ¨ Whatâ€™s Image Processing?
Image processing is like giving your image a makeover using filters! These filters change the pixels (tiny dots of color) to create cool effects like sharpening, blurring, or highlighting edges.

## ğŸ§° Whatâ€™s a Filter?
A filter uses a tiny grid of numbers called a kernel. Think of it like a stencil that slides over your image and changes it based on math.

Hereâ€™s an example of a 3x3 kernel:

-1 -1 -1  
-1  8 -1  
-1 -1 -1

This one is called a Laplace filterâ€”it helps find the edges in an image.

## ğŸ–¼ï¸ Letâ€™s Try It!
Hereâ€™s a simple grayscale image (black = 0, white = 255):

 0   0   0   0   0   0   0  
 0   0   0   0   0   0   0  
 0   0 255 255 255  0   0  
 0   0 255 255 255  0   0  
 0   0 255 255 255  0   0  
 0   0   0   0   0   0   0  
 0   0   0   0   0   0   0
Now we slide the filter over the image and do some math:

ğŸ” First Patch:
(0Ã—-1) + (0Ã—-1) + (0Ã—-1) +  
(0Ã—-1) + (0Ã—8) + (0Ã—-1) +  
(0Ã—-1) + (0Ã—-1) + (255Ã—-1) = -255
ğŸ‘‰ Next Patch:
(0Ã—-1) + (0Ã—-1) + (0Ã—-1) +  
(0Ã—-1) + (0Ã—8) + (0Ã—-1) +  
(0Ã—-1) + (255Ã—-1) + (255Ã—-1) = -510
So far, our new image looks like this:

-255  -510
We keep sliding the filter across the image until weâ€™ve covered it all!

![Filter](../assets/filter.gif)

## ğŸ§  Whatâ€™s Happening?
The filter highlights edges in the image.
Some values might go below 0 or above 255, so we adjust them to stay in range.
The edges of the image donâ€™t get calculated, so we pad them with 0s.

## ğŸŒ Real Example
![Original Image](../assets/banana-grayscale.png)
![Filtered Image](../assets/laplace.png)
Banana photo	Banana with edges highlighted
This is called convolutional filteringâ€”because the filter is convolved (slid and calculated) across the image.

## ğŸ§ª Whatâ€™s Next?
Letâ€™s connect this to how modern vision models (like AI that sees) use convolutional filtering to understand images!

# ğŸ§  From Filters to Smart Vision: How Computers Learn to See  
**ğŸ¯ Level Up Your AI Skills**  
**â±ï¸ 5â€“7 minutes**

---

## ğŸ¨ Filters Are Cool, But Seeing Is Smarter

Sure, filters can make your pics look awesomeâ€”sharper, blurrier, or more artsy. But in **computer vision**, the goal isnâ€™t just to make images look cool. Itâ€™s to help computers **understand whatâ€™s in the image**â€”like spotting a banana or a catâ€”and make smart decisions based on that.

---

> ğŸ’¡ **Heads up!**  
> This section assumes you already know the basics of **machine learning** and have heard of **deep learning** and **neural networks**.  
> If not, check out the https://learn.microsoft.com/training/modules/create-regression-models-azure-machine-learning/ on Microsoft Learn!

---

## ğŸ§  Whatâ€™s a CNN?

A **Convolutional Neural Network (CNN)** is a special kind of deep learning model thatâ€™s super good at looking at images.

Hereâ€™s how it works:

1. **Filters** scan the image and pull out important details (like edges, shapes, or colors).
2. These details become **numbers** (called feature maps).
3. The numbers go into a **neural network** that figures out what the image is.

### ğŸ Example:
You train a CNN with lots of fruit picsâ€”apples, bananas, oranges.  
Then, when you show it a new fruit pic, it says:  
> â€œHey, thatâ€™s a banana!â€ ğŸŒ

![Convolutional](../assets/convolutional-neural-network.png)

## ğŸ› ï¸ How CNNs Learn

When training starts, the filters are just random numbers. But as the model sees more images and compares its guesses to the correct answers, it **tweaks the filters** to get better.

### ğŸ” Training Loop:
- Show the model a labeled image (like â€œthis is an orangeâ€).
- The model makes a guess (maybe â€œbananaâ€).
- It checks how wrong it was.
- It adjusts the filters and tries again.
- Repeat this over and over (called **epochs**) until it gets really good.

---

## ğŸ§ª Whatâ€™s Inside a CNN?

Hereâ€™s a simplified breakdown:

1. **Input**: Images with labels (like 0 = apple, 1 = banana, 2 = orange).
2. **Convolution Layers**: Filters scan the image and create feature maps.
3. **Flattening**: The feature maps are turned into a long list of numbers.
4. **Fully Connected Layers**: These numbers go into a neural network.
5. **Output**: The model gives a probability for each label, like:
   ```

   [0.2, 0.5, 0.3] â†’ Most likely a banana!
   
6. **Loss Calculation**: The model compares its guess to the real answer and learns from the mistake.

---

## ğŸ§  Bonus Tip

Real CNNs have **lots of layers**â€”some shrink the image, some clean up the data, and others help the model focus on the most important parts. But the key idea is this:

> **Filters turn images into numbers. Neural networks turn numbers into answers.**

---

## ğŸš€ Whatâ€™s Next?

Now that you know how CNNs work, letâ€™s see how they power real-world computer vision tasksâ€”like facial recognition, self-driving cars, and more!

# ğŸ‘ï¸â€ğŸ—¨ï¸ Understand Modern Vision Models  

## ğŸ“¸ CNNs: The OG of Computer Vision

CNNs (Convolutional Neural Networks) have been the go-to tech for helping computers â€œseeâ€ for years. Theyâ€™re great at figuring out whatâ€™s in a pictureâ€”like â€œthatâ€™s a dogâ€ or â€œthatâ€™s a skateboard.â€

But CNNs can do more than just label stuff. Theyâ€™re also used in **object detection**, which means spotting **multiple things** in one image and figuring out **where** they are.

---

## ğŸ¤– Transformers: From Text to Vision
While CNNs ruled vision, **transformers** were crushing it in **language** (like chatbots and translation). Transformers read tons of text and turn words into **vectors**â€”basically, numbers that represent meaning.

![Language encoder](../assets/language-encoder.png)

### ğŸ§  Word Vibes in 3D
Imagine words floating in 3D space. Words that mean similar things (like â€œcatâ€ and â€œkittenâ€) are close together. Thatâ€™s how transformers understand language.

> âš ï¸ Real transformers use way more than 3 dimensionsâ€”like hundreds! But 3D is easier to picture.

---

## ğŸ§© Multi-Modal Models: Best of Both Worlds

Now imagine combining **image understanding** with **language understanding**. Thatâ€™s what **multi-modal models** do!

Theyâ€™re trained on **captioned images** (like a pic of a dog with the caption â€œcute puppyâ€). These models learn how **pixels** and **words** connect.

!Multi Modal](/../assets/multi-modal-model.png)

### ğŸ§  How It Works:
- An **image encoder** turns the picture into numbers.
- A **language encoder** turns the caption into numbers.
- The model learns how they match up.

<video src="../assets/20250707-124105-sora.mp4" controls autoplay loop muted playsinline style="max-width: 100%; height: auto;">
  Your browser does not support the video tag.
</video>

## ğŸ—ï¸ Foundation Models: Build Once, Use Everywhere

Modern vision models are trained on **huge datasets** from the internet. These are called **foundation models**â€”theyâ€™re like the base of a LEGO set.

You can build different tools on top of them, like:

- ğŸ–¼ï¸ **Image Classification** â€“ What is this image?
- ğŸ” **Object Detection** â€“ Where are the things in this image?
- ğŸ“ **Captioning** â€“ Describe this image in words.
- ğŸ·ï¸ **Tagging** â€“ Add relevant keywords to the image.

![Florence](../assets/florence-model.png)
---

## ğŸš€ Why It Matters

Multi-modal models are the **future of AI**. Theyâ€™re helping us build smarter tools that can understand both **what they see** and **what they read**â€”just like humans.

# ğŸ§  Computer Vision Quiz

---

## 1. What kind of values does computer vision analyze in an image?

- [ ] Timestamps in photograph metadata  
- [x] Pixels  
- [ ] Image file names  

---

## 2. What is the primary role of filters in a convolutional neural network (CNN) used for image classification?

- [ ] To apply visual effects to enhance image appearance  
- [x] To extract numeric features from images for use in a neural network  
- [ ] To compress image size for faster processing  

---

## 3. What is the primary function of a multi-modal model in computer vision?

- [ ] To generate random captions for unlabeled images  
- [ ] To replace CNNs entirely in all vision tasks  
- [x] To combine image features with natural language embeddings for richer understanding  


